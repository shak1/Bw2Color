{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bw2Color_Pix2Pix",
      "provenance": [],
      "collapsed_sections": [
        "MTfkcyJ_-DWs",
        "kHiFF-bs-rLZ",
        "_0HBzw8aGTbx"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTfkcyJ_-DWs"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw45vVGFdFdp"
      },
      "source": [
        "!pip install -q -U progressbar2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceXOdXEIcqrt"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, Dropout, Layer, MaxPooling2D\n",
        "from tensorflow.keras.layers import ReLU, Input, Conv2DTranspose, Concatenate, ZeroPadding2D\n",
        "from tensorflow.keras import  Model\n",
        "import os\n",
        "from math import log2\n",
        "import time\n",
        "from skimage import  color, io\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import numpy as np\n",
        "from keras.callbacks import *\n",
        "from IPython import display\n",
        "from IPython.display import clear_output\n",
        "import shutil\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "plt.ioff()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "512oCoGLdQg-"
      },
      "source": [
        "#Buffer size of the input pipeline\n",
        "BUFFER_SIZE = 50000\n",
        "#Batch size\n",
        "BATCH_SIZE= 4\n",
        "#Number of epochs\n",
        "EPOCHS = 100\n",
        "#Image size\n",
        "IMG_SIZE = 128\n",
        "#Number of output channel(in this case 2)\n",
        "OUTPUT_CHANNELS = 2\n",
        "#Lambda value of the generator function\n",
        "LAMBDA = 10\n",
        "#FOlder containing the datasets.\n",
        "DATASET_FOLDER = '/content/drive/My Drive/Bw2Color/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toX58bLj-eNP"
      },
      "source": [
        "Download the training/val dataset from the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47MZB9LMvgLQ",
        "outputId": "a1218808-78ed-40ec-a67f-237596fd5260"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjIXznY5eIHy"
      },
      "source": [
        "shutil.copy(DATASET_FOLDER+\"Train.zip\", \".\")\n",
        "!unzip -q Train.zip\n",
        "!rm Train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i5rEK-grh7U"
      },
      "source": [
        "shutil.copy(DATASET_FOLDER+\"Val.zip\", \".\")\n",
        "!unzip -q Val.zip\n",
        "!rm Val.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHiFF-bs-rLZ"
      },
      "source": [
        "# Data preprocessing\n",
        "\n",
        "Here we define the necessary functions for the online preprocessinr steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6MT9V9mAD5l"
      },
      "source": [
        "\"\"\"\n",
        "Convert a bath of images from the RGB space to LAB space\n",
        "(Code adapted from:https://github.com/xahidbuffon/TF_RGB_LAB/blob/master/rgb_lab_formulation.py).\n",
        "\"\"\"\n",
        "def convert2lab(img):\n",
        "  srgb_pixels = tf.reshape(img, [-1, 3])\n",
        "  \n",
        "  linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)\n",
        "  exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)\n",
        "  rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask\n",
        "  rgb_to_xyz = tf.constant([\n",
        "      #    X        Y          Z\n",
        "      [0.412453, 0.212671, 0.019334], # R\n",
        "      [0.357580, 0.715160, 0.119193], # G\n",
        "      [0.180423, 0.072169, 0.950227], # B\n",
        "  ])\n",
        "  xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)\n",
        " \n",
        "# https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "  xyz_normalized_pixels = tf.multiply(xyz_pixels, [1/0.950456, 1.0, 1/1.088754])\n",
        " \n",
        "  epsilon = 6/29\n",
        "  linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon**3), dtype=tf.float32)\n",
        "  exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon**3), dtype=tf.float32)\n",
        "  fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon**2) + 4/29) * linear_mask + (xyz_normalized_pixels ** (1/3)) * exponential_mask\n",
        " \n",
        "  # convert to lab\n",
        "  fxfyfz_to_lab = tf.constant([\n",
        "      #  l       a       b\n",
        "      [  0.0,  500.0,    0.0], # fx\n",
        "      [116.0, -500.0,  200.0], # fy\n",
        "      [  0.0,    0.0, -200.0], # fz\n",
        "  ])\n",
        "  lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])\n",
        " \n",
        "  return tf.reshape(lab_pixels, tf.shape(img))\n",
        "\n",
        "\"\"\"\n",
        "Convert a bath of images from the LAB space to  space\n",
        "(Code adapted from:https://github.com/xahidbuffon/TF_RGB_LAB/blob/master/rgb_lab_formulation.py).\n",
        "\"\"\"\n",
        "\n",
        "def convert2rgb(lab):\n",
        "  lab_pixels = tf.reshape(lab, [-1, 3])\n",
        "# https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "  # convert to fxfyfz\n",
        "  lab_to_fxfyfz = tf.constant([\n",
        "      #   fx      fy        fz\n",
        "      [1/116.0, 1/116.0,  1/116.0], # l\n",
        "      [1/500.0,     0.0,      0.0], # a\n",
        "      [    0.0,     0.0, -1/200.0], # b\n",
        "  ])\n",
        "  fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)\n",
        "\n",
        "  # convert to xyz\n",
        "  epsilon = 6/29\n",
        "  linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)\n",
        "  exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)\n",
        "  xyz_pixels = (3 * epsilon**2 * (fxfyfz_pixels - 4/29)) * linear_mask + (fxfyfz_pixels ** 3) * exponential_mask\n",
        "\n",
        "    # denormalize for D65 white point\n",
        "  xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])\n",
        "\n",
        "      \n",
        "  xyz_to_rgb = tf.constant([\n",
        "      #     r           g          b\n",
        "      [ 3.2404542, -0.9692660,  0.0556434], # x\n",
        "      [-1.5371385,  1.8760108, -0.2040259], # y\n",
        "      [-0.4985314,  0.0415560,  1.0572252], # z\n",
        "  ])\n",
        "  rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)\n",
        "  # avoid a slightly negative number messing up the conversion\n",
        "  rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)\n",
        "  linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)\n",
        "  exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)\n",
        "  srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + ((rgb_pixels ** (1/2.4) * 1.055) - 0.055) * exponential_mask\n",
        "\n",
        "  return tf.reshape(srgb_pixels, tf.shape(lab))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Iy5SyOdXTx"
      },
      "source": [
        "\"\"\"\n",
        "Resize an image so that the shortest side is of the given size while maintaing the\n",
        "aspect-ration.\n",
        "\"\"\"\n",
        "def resize_with_ratio(img, size):\n",
        "  #retrieve the size of the image.\n",
        "  w, h = tf.shape(img)[0], tf.shape(img)[1]\n",
        " \n",
        "  #calculate the new size.\n",
        "  new_height = h * size // tf.minimum(w, h)\n",
        "  new_width = w * size // tf.minimum(w, h)\n",
        "  \n",
        "  #reize the image.\n",
        "  img = tf.image.resize(img, [new_width, new_height], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  return img\n",
        " \n",
        "\n",
        "\"\"\"\n",
        "Extract a square patch of the given size from the center of the image.\n",
        "\"\"\"\n",
        "def center_crop(img, size):\n",
        "  #retrieve the size of the image.\n",
        "  w, h = tf.shape(img)[0], tf.shape(img)[1]\n",
        "\n",
        "  #calculate the left-uppermost coordinates of the pacth being extraxted.\n",
        "  x = (w - IMG_SIZE)//2\n",
        "  y = (h - IMG_SIZE)//2\n",
        "\n",
        "  #crop the central patch.\n",
        "  img = tf.image.crop_to_bounding_box(img, x, y, IMG_SIZE, IMG_SIZE)\n",
        "  return img\n",
        " \n",
        "\"\"\"\n",
        "Normalize a batch images in lab space so that each pixel is in the range [-1, 1].\n",
        "\"\"\"\n",
        "def normalise(lab_imgs):\n",
        "  #reshape the batch of images to (batch*height*width, num_channel)..\n",
        "  imgs = tf.reshape(lab_imgs, [-1, 3])\n",
        "  #nomarmalise the values.\n",
        "  normalised = (imgs / [50.0, 110.0, 110.0] ) - [1.0, 0.0, 0.0]\n",
        "  #reshape the array to the original shape.\n",
        "  return tf.reshape(normalised, tf.shape(lab_imgs))\n",
        "\n",
        "\"\"\"\n",
        "Denormalise values of a batch of images in lab space to the original range.\n",
        "\"\"\"\n",
        "def denormalise(lab_imgs):\n",
        "  #reshape the batch of images to (batch*height*width, num_channel).\n",
        "  imgs = tf.reshape(lab_imgs, [-1, 3])\n",
        "  #nomarmalise the values.\n",
        "  unormalised = (imgs + [1.0, 0.0, 0.0]) * [50.0, 110.0, 110.0]\n",
        "  #reshape the array to the original shape.\n",
        "  return tf.reshape(unormalised, tf.shape(lab_imgs))\n",
        "\n",
        "\"\"\"\n",
        "Randomly flip the image on the vertical axis.\n",
        "\"\"\"\n",
        "def data_augmentation(img):\n",
        "  img = tf.image.random_flip_left_right(img)\n",
        "  return img\n",
        "\n",
        "\"\"\"\n",
        "Recontruct the RGB image given the L-channel and AB-channels.\n",
        "Values are in the range [0, 1] if the rescale is Fasle, otherwise they are\n",
        "rescaled to the range [0, 255].\n",
        "\"\"\"\n",
        "def post_process(L, AB, rescale=False):\n",
        "  #concatenate the L and AB channels.\n",
        "  image_lab = tf.concat([L, AB], axis=-1)\n",
        "  #denormalise the values.\n",
        "  image_lab = denormalise(image_lab)\n",
        "  #convert the image back to the rgb space.\n",
        "  img_rgb = convert2rgb(image_lab)\n",
        "  #rescale the values to the range [0, 255] if required.\n",
        "  if rescale:\n",
        "    img_rgb = img_rgb*255.0\n",
        "\n",
        "  return img_rgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0HBzw8aGTbx"
      },
      "source": [
        "# Input pipeline\n",
        "\n",
        "Here we create the input pipeline for the trainig dataset and validation dataset which will be used to generated images during the training process in order to assess the performance of the generator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqKJzF-5dp_S"
      },
      "source": [
        "\"\"\"\n",
        "Load the image.\n",
        "\"\"\"\n",
        "def load(image_path):\n",
        "  #read the content of the file.\n",
        "  img = tf.io.read_file(image_path)\n",
        "  #decode the content of the file as jpeg image.\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  #scale the values so that the fall in the range [0, 1].\n",
        "  img = tf.cast(img, dtype=tf.float32)/255.0\n",
        "  return img\n",
        " \n",
        "\"\"\"\n",
        "Load an image and apply the preprocess step.\n",
        "Return a tuple made of the luminance channel L, the chromatic channels AB\n",
        "and the path of the image.\n",
        "\"\"\"\n",
        "def load_image_train(file_path):\n",
        "  #load the image.\n",
        "  img = load(file_path)\n",
        "  #apply data augmentation\n",
        "  img = tf.image.random_flip_left_right(img)\n",
        "  #convert to lab space.\n",
        "  lab_img = convert2lab(img)\n",
        "  #normalise the values.\n",
        "  lab_img = normalise(lab_img)\n",
        "  #extract the luminance channel.\n",
        "  L = lab_img[:,:,0][..., tf.newaxis]\n",
        "  #extract the chromatic channels\n",
        "  AB = tf.stack([lab_img[:,:,1], lab_img[:,:,2]], axis=-1)\n",
        "\n",
        "  return L, AB, file_path\n",
        " \n",
        " \"\"\"\n",
        "Load an image and apply the preprocess step without the data augmentation.\n",
        "Return a tuple made of the luminance channel L and the path of the image.\n",
        " \"\"\"\n",
        "\n",
        "#load an image and cretes the couple (L, RGB)\n",
        "def load_image_test(file_path):\n",
        "  #load image as RGB.\n",
        "  img = load(file_path)\n",
        "  #Convert the image to LAB Space.\n",
        "  lab_img = convert2lab(img).\n",
        "  #Nomalise the values so that they are in the range [-1, -1]\n",
        "  lab_img = normalise(lab_img)\n",
        "  #Extract the L channel.\n",
        "  L = lab_img[:,:,0][..., tf.newaxis]\n",
        "  return L, img, file_path\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "#create the training dataset pipeline.\n",
        "train_dataset = tf.data.Dataset.list_files('/content/Train/*.jpg')\n",
        "                                              \n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\\\n",
        "                            .map(load_image_train, num_parallel_calls=AUTOTUNE)\\\n",
        "                            .batch(BATCH_SIZE)\\\n",
        "                            .prefetch(AUTOTUNE)\n",
        "\n",
        "#create the validation dataset pipeline.\n",
        "val_dataset = tf.data.Dataset.list_files('/content/Val/*.jpg', shuffle=False)\n",
        "                                              \n",
        "val_dataset = val_dataset.map(load_image_test)\\\n",
        "                          .batch(BATCH_SIZE)\\\n",
        "                          .prefetch(AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2KSN6z0JwLb"
      },
      "source": [
        "# Building blocks\n",
        "\n",
        "Here we create the building blocks that is the _Conv-BatchNorm-ReLu_ and _Conv-BatchNorm-Dropout-ReLu_ modulues that compose the encoder and decoder of the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqjWABii_OSl"
      },
      "source": [
        "\"\"\"\n",
        "Define the Conv-BatchNorm-ReLu block for the encoder part of the generator.\n",
        "\"\"\"\n",
        "def down_sample(input, filters, apply_batchnorm=True):\n",
        "  #initialize the weight with a normal distribution with mean 0 and standard deviation on 0.02.\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  #apply the convultion layer.\n",
        "  x = Conv2D(filters, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False)(input)\n",
        "  #apply the batch normalization layer if required.\n",
        "  if apply_batchnorm:\n",
        "    x = BatchNormalization()(x,training=True)\n",
        "  #apply the leaky relu activation function.\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "\"\"\"\n",
        "Define the Conv-BatchNorm-Droput-ReLu block for the decoder part of the generator.\n",
        "\"\"\"\n",
        "def up_sample(input, skip, filters, apply_dropout=False):\n",
        "  #initialize the weight with a normal distribution with mean 0 and standard deviation on 0.02.\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  #apply the transposed convultion layer.\n",
        "  x = Conv2DTranspose(filters, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False)(input)\n",
        "  #apply the batch normalization layer.\n",
        "  x = BatchNormalization()(x,training=True)\n",
        "  #apply the dropout layer if required.\n",
        "  if apply_dropout:\n",
        "    x = Dropout(0.5)(x, training=True)\n",
        "  #apply the ReLu activation function.\n",
        "  x = ReLU()(x)\n",
        "  #concatenate the activations if i and n-i layers.\n",
        "  x = Concatenate()([x, skip])\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGpWRqs5HfMo"
      },
      "source": [
        "# Generator\n",
        "\n",
        "The Generator has encoder-decoder architecture with skip connection between the _i_-th layer and _n-i_-th layer of the model. Each block of the encoder has the from of _Conv->Batchnorm->Leaky ReLU_ and while the decoder has blocks of the from _Transposed Conv -> Batchnorm -> Dropout(applied to the first 2 blocks) -> ReLU_. It takes in input the luminance L channel as a single channel 128x128x1 image and produce the chromatic _a_ and _b_ channels as 128x128x2 image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjLXtuSafGPt"
      },
      "source": [
        "def define_generator(input_shape, output_channel, n_blocks):\n",
        "  #initialize the weight with a normal distribution with mean 0 and standard deviation on 0.02.\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  #define the input.\n",
        "  inputs = Input(input_shape)\n",
        "\n",
        "  #the number of initial filters.\n",
        "  min_filters = 64\n",
        "  \n",
        "  #build the encoder part of the generator.\n",
        "  x = down_sample(inputs, 64, apply_batchnorm=False)\n",
        "  skips = [x]\n",
        "  nb_filters = [min_filters]\n",
        "  for i in range(1, n_blocks - 1):\n",
        "    filters = min_filters * min(8, (2 ** i))\n",
        "    x = down_sample(x, filters)\n",
        "    skips.append(x)\n",
        "    nb_filters.append(filters)\n",
        "  \n",
        "  #central block of the encoder-decoder \n",
        "  x = down_sample(x, 512, apply_batchnorm=False)\n",
        "\n",
        "  #build the decoder part of the generator. \n",
        "  nb_filters = reversed(nb_filters)\n",
        "  skips = reversed(skips)\n",
        "  for i, (filters, skip) in enumerate(zip(nb_filters, skips)):\n",
        "    apply_dp = i < 2\n",
        "    x = up_sample(x, skip, filters, apply_dropout=apply_dp)\n",
        "\n",
        "  #the output layer\n",
        "  output = Conv2DTranspose(output_channel, \n",
        "                          4, \n",
        "                          strides=2, \n",
        "                          padding='same', \n",
        "                          kernel_initializer=initializer,\n",
        "                          activation='tanh')(x) \n",
        "  #return the\n",
        "  return Model(inputs=inputs, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFn-Km-6L0Vi"
      },
      "source": [
        "# Discriminator\n",
        "\n",
        "The discriminator takes in input the luminance L channel as a single channel 128x128x1 image and the chromatic a and b channels as a 128x128x2 image. This two inputs are then concatenated to form the final image which the discriminator will classify by classifing at its 70x70 patches. It is composed by modules of the from Conv->Batchnorm->Leaky ReLU and its out is a (batch, 14, 14, 1) vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwOKFUeRfWM5"
      },
      "source": [
        "def define_discriminator(input_shape, target_shape):\n",
        "  #initialize the weight with a normal distribution with mean 0 and standard deviation on 0.02.\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  #define the input that containes the luminance channel.\n",
        "  inp = Input(shape=input_shape, name='input_image')\n",
        "  #define the input that containes the chromatic channels.\n",
        "  tar = Input(shape=target_shape, name='target_image')\n",
        "  #concatenate the inputs\n",
        "  x = Concatenate()([inp, tar])\n",
        "\n",
        "  x = down_sample(x, 64, apply_batchnorm=False)\n",
        "  x = down_sample(x, 128)\n",
        "  x = down_sample(x, 256) \n",
        "\n",
        "  x = ZeroPadding2D()(x)\n",
        "  x = Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(x)\n",
        "\n",
        "  x = BatchNormalization()(x, training=True)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = ZeroPadding2D()(x)\n",
        "\n",
        "  last = Conv2D(1, 4, strides=1, kernel_initializer=initializer)(x)\n",
        "\n",
        "  return Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmXZgz7mUAJk"
      },
      "source": [
        "# Losses and Optimzer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AICkxOnfmxo"
      },
      "source": [
        "#use the mean squared error loss for the adversarial loss.\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "#define the optimzer for the generator.\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
        "#define the optimzer for the discriminator.\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ1TJGZKfMpg"
      },
      "source": [
        "\"\"\"\n",
        "Calculate the loss for the generator.\n",
        "\"\"\"\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  #adversarial loss\n",
        "  gan_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        " \n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "  \n",
        "  #the total loss is the sum of the adversarial loss and L1 distance between the target and generated output.\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        " \n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TvqEEmXf1tv"
      },
      "source": [
        "\"\"\"\n",
        "Calculate the loss for the discriminator.\n",
        "\"\"\"\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  #adversarial loss associated to the real data.\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "  \n",
        "  #adversarial loss associated to the fake data.\n",
        "  generated_loss = loss_object(tf.ones_like(disc_generated_output)*-1.0, disc_generated_output)\n",
        " \n",
        "  \n",
        "  total_disc_loss = real_loss + generated_loss\n",
        " \n",
        "  return total_disc_loss * 0.5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0eMDXlWf2OL"
      },
      "source": [
        "import datetime\n",
        "log_dir=\"logs/\"\n",
        " \n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tXgNh9RP7md"
      },
      "source": [
        "def generate_images(dataset, generator, save_path):\n",
        "  \n",
        "  def generate(dataset, generator):\n",
        "    batch_shape = tf.data.experimental.get_structure(dataset)[1].shapREAL_PATHe\n",
        "    fake_images = np.empty(shape=(0, IMG_SIZE, IMG_SIZE, 3))\n",
        "    inputs = np.empty(shape=(0, IMG_SIZE, IMG_SIZE, 1))\n",
        "    real_images = np.empty_like(fake_images)\n",
        "\n",
        "    for Ls, real, _ in dataset:\n",
        "      fake_ABs = generator.predict(Ls)\n",
        "\n",
        "      fake_RGBs = post_process(Ls, fake_ABs)\n",
        "\n",
        "      fake_images = np.append(fake_images, fake_RGBs, axis=0)\n",
        "      inputs = np.append(inputs, Ls, axis=0)\n",
        "      real_images = np.append(real_images, real, axis=0)\n",
        "\n",
        "    return inputs, real_images, fake_images \n",
        "\n",
        "  fig =  plt.figure(dpi=300, figsize=(30,30), constrained_layout=False)\n",
        "\n",
        "  inputs, real_images, fake_images = generate(dataset, generator)\n",
        "  num_images = fake_images.shape[0]\n",
        "\n",
        "  grid = ImageGrid(fig, 141,  #\n",
        "                     nrows_ncols=(num_images, 3),\n",
        "                     axes_pad=0.01,\n",
        "                     label_mode=\"1\",\n",
        "                     )\n",
        "\n",
        "  title = ['Input','Real', 'Fake']\n",
        "\n",
        "  for n, (L, real, fake) in enumerate(zip(inputs, real_images, fake_images)):\n",
        "    input = (L[..., 0] + 1.0)/2.0\n",
        "    display_list = [input , real, fake]\n",
        "\n",
        "    for i in range(3):\n",
        "      ax = grid[n*3 + i]\n",
        "      ax.axis('off')\n",
        "      if n == 0:\n",
        "        ax.set_title(title[i])\n",
        "      if i == 0:\n",
        "        ax.imshow(display_list[i], cmap='gray', aspect='auto')\n",
        "      else:\n",
        "        ax.imshow(display_list[i], aspect='auto')\n",
        "      \n",
        "  fig.savefig(save_path, bbox_inches='tight', aspect='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo5mQ3W8jtId"
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, epoch):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image)\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target])\n",
        "    disc_generated_output = discriminator([input_image, gen_output])\n",
        " \n",
        "    gen_total_loss, gen_gan_loss, l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        " \n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        " \n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        " \n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_l1_loss', l1_loss, step=epoch)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jrXLaeLj0d_"
      },
      "source": [
        "generator = define_generator([IMG_SIZE, IMG_SIZE, 1], OUTPUT_CHANNELS, int(log2(IMG_SIZE)))\n",
        "discriminator = define_discriminator([IMG_SIZE, IMG_SIZE, 1], [IMG_SIZE, IMG_SIZE, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a3FdfZTk7J6"
      },
      "source": [
        "checkpoint_dir = '/content/drive/My Drive/VCS Project/checkpoints'\n",
        "checkpoints = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator,\n",
        "                                 step=tf.Variable(0))\n",
        "manager = tf.train.CheckpointManager(checkpoints, checkpoint_dir, max_to_keep=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0FbjAJoj-99"
      },
      "source": [
        "import datetime\n",
        "from datetime import timedelta\n",
        "import progressbar\n",
        "\n",
        "widgets=[\n",
        "    progressbar.Bar(marker='=', left='[', right=']'),\n",
        "    progressbar.widgets.SimpleProgress(),\n",
        "    ' - ', progressbar.Timer(), '',\n",
        "    '-', progressbar.AdaptiveETA(), ''\n",
        "]\n",
        "def fit(train_ds, epochs):\n",
        "  checkpoints.restore(manager.latest_checkpoint)\n",
        "\n",
        "  if manager.latest_checkpoint:\n",
        "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "  else:\n",
        "    print(\"Initializing from scratch.\")\n",
        "\n",
        "  step = int(checkpoints.step)\n",
        "  \n",
        "  for epoch in range(step, epochs):\n",
        "    start = time.time()\n",
        "    iters = train_ds.cardinality().numpy()\n",
        " \n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    # Train\n",
        "    with progressbar.ProgressBar(max_value=iters, widgets=widgets) as pbar:\n",
        "      for n, (input_image, target, _) in train_ds.enumerate():\n",
        "    \n",
        "        train_step(input_image, target, epoch)\n",
        "        pbar.update(n.numpy())\n",
        "\n",
        "    path = f'/content/drive/My Drive/VCS Project/samples/sample_{epoch + 1}.jpg'\n",
        "    generate_images(val_dataset, generator , path);\n",
        "\n",
        "    checkpoints.step.assign_add(1)\n",
        "    #saving (checkpoint) the model after 10 epochs\n",
        "    if epoch >= 10:\n",
        "      manager.save()\n",
        "\n",
        "    clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayc7S9hWIIOl"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ainz9burlNU9"
      },
      "source": [
        "fit(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVMEkgZZhaaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b767d5-502d-4e29-b987-f12d7e508005"
      },
      "source": [
        "checkpoints.restore('/content/drive/My Drive/VCS Project/Best Model/ckpt-97')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbf3ff597f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qM7pHucDUXI"
      },
      "source": [
        "path = f'/content/drive/My Drive/VCS Project/sample.jpg'\n",
        "generate_images(val_dataset, generator , path);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTu3c7ewBDz-",
        "outputId": "f00b6599-3803-4fc0-e2ca-08c825e50139"
      },
      "source": [
        "generator.save('/content/drive/My Drive/VCS Project/Best model/', include_optimizer=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/VCS Project/Best model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJnI8dtVTYWg"
      },
      "source": [
        "shutil.copy(\"/content/drive/My Drive/VCS Project/Test.zip\", \".\")\n",
        "!unzip -q Test.zip\n",
        "!rm Test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk7SjqZxlbG5"
      },
      "source": [
        "test_paths = tf.data.Dataset.list_files('/content/Test/*.jpg', shuffle=False)\n",
        "test_dataset = test_paths.map(load_image_test, deterministic=True)\\\n",
        "                          .batch(BATCH_SIZE)\\\n",
        "                          .prefetch(AUTOTUNE)\n",
        "\n",
        "for Ls, _, file_paths in test_dataset:\n",
        "\n",
        "  fake_ABs = generator.predict(Ls)\n",
        "\n",
        "  fake_LABs = tf.concat([Ls, fake_ABs], axis=-1)\n",
        "  fake_LABs = denormalise(fake_LABs)\n",
        "  fake_RGBs = convert2rgb(fake_LABs)\n",
        "\n",
        "  for (fake, file_path) in zip(fake_RGBs, file_paths):\n",
        "    fake_img = tf.image.convert_image_dtype(fake, dtype=tf.uint8)\n",
        "    img = tf.image.encode_jpeg(fake_img)\n",
        "\n",
        "    filename = os.path.basename(file_path.numpy())\n",
        "    save_path= f'/content/drive/My Drive/VCS Project/examples/{filename.decode(\"utf-8\")}'\n",
        "    tf.io.write_file(save_path, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgzjIkPVj-E-"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZMw1fUMyssI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}