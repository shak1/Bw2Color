{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bw2Color_Baseline",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw45vVGFdFdp"
      },
      "source": [
        "!pip install -q -U progressbar2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceXOdXEIcqrt"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, Dropout, Layer, MaxPooling2D\n",
        "from tensorflow.keras.layers import ReLU, Input, Conv2DTranspose, Concatenate, ZeroPadding2D\n",
        "from tensorflow.keras import  Model\n",
        "import os\n",
        "from math import log2\n",
        "import time\n",
        "from skimage import  color, io\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import numpy as np\n",
        "from keras.callbacks import *\n",
        "from IPython import display\n",
        "from IPython.display import clear_output\n",
        "import shutil\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "plt.ioff()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "512oCoGLdQg-"
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE= 4\n",
        "EPOCHS = 100\n",
        "IMG_SIZE = 128\n",
        "OUTPUT_CHANNELS = 2\n",
        "LAMBDA = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47MZB9LMvgLQ",
        "outputId": "a1218808-78ed-40ec-a67f-237596fd5260"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjIXznY5eIHy"
      },
      "source": [
        "shutil.copy(\"/content/drive/My Drive/VCS Project/Train.zip\", \".\")\n",
        "!unzip -q Train.zip\n",
        "!rm Train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i5rEK-grh7U"
      },
      "source": [
        "shutil.copy(\"/content/drive/My Drive/VCS Project/Val.zip\", \".\")\n",
        "!unzip -q Val.zip\n",
        "!rm Val.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6MT9V9mAD5l"
      },
      "source": [
        "def convert2lab(img):\n",
        "  srgb_pixels = tf.reshape(img, [-1, 3])\n",
        "  \n",
        "  linear_mask = tf.cast(srgb_pixels <= 0.04045, dtype=tf.float32)\n",
        "  exponential_mask = tf.cast(srgb_pixels > 0.04045, dtype=tf.float32)\n",
        "  rgb_pixels = (srgb_pixels / 12.92 * linear_mask) + (((srgb_pixels + 0.055) / 1.055) ** 2.4) * exponential_mask\n",
        "  rgb_to_xyz = tf.constant([\n",
        "      #    X        Y          Z\n",
        "      [0.412453, 0.212671, 0.019334], # R\n",
        "      [0.357580, 0.715160, 0.119193], # G\n",
        "      [0.180423, 0.072169, 0.950227], # B\n",
        "  ])\n",
        "  xyz_pixels = tf.matmul(rgb_pixels, rgb_to_xyz)\n",
        " \n",
        "# https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "  xyz_normalized_pixels = tf.multiply(xyz_pixels, [1/0.950456, 1.0, 1/1.088754])\n",
        " \n",
        "  epsilon = 6/29\n",
        "  linear_mask = tf.cast(xyz_normalized_pixels <= (epsilon**3), dtype=tf.float32)\n",
        "  exponential_mask = tf.cast(xyz_normalized_pixels > (epsilon**3), dtype=tf.float32)\n",
        "  fxfyfz_pixels = (xyz_normalized_pixels / (3 * epsilon**2) + 4/29) * linear_mask + (xyz_normalized_pixels ** (1/3)) * exponential_mask\n",
        " \n",
        "  # convert to lab\n",
        "  fxfyfz_to_lab = tf.constant([\n",
        "      #  l       a       b\n",
        "      [  0.0,  500.0,    0.0], # fx\n",
        "      [116.0, -500.0,  200.0], # fy\n",
        "      [  0.0,    0.0, -200.0], # fz\n",
        "  ])\n",
        "  lab_pixels = tf.matmul(fxfyfz_pixels, fxfyfz_to_lab) + tf.constant([-16.0, 0.0, 0.0])\n",
        " \n",
        "  return tf.reshape(lab_pixels, tf.shape(img))\n",
        "\n",
        "def convert2rgb(lab):\n",
        "  lab_pixels = tf.reshape(lab, [-1, 3])\n",
        "# https://en.wikipedia.org/wiki/Lab_color_space#CIELAB-CIEXYZ_conversions\n",
        "  # convert to fxfyfz\n",
        "  lab_to_fxfyfz = tf.constant([\n",
        "      #   fx      fy        fz\n",
        "      [1/116.0, 1/116.0,  1/116.0], # l\n",
        "      [1/500.0,     0.0,      0.0], # a\n",
        "      [    0.0,     0.0, -1/200.0], # b\n",
        "  ])\n",
        "  fxfyfz_pixels = tf.matmul(lab_pixels + tf.constant([16.0, 0.0, 0.0]), lab_to_fxfyfz)\n",
        "\n",
        "  # convert to xyz\n",
        "  epsilon = 6/29\n",
        "  linear_mask = tf.cast(fxfyfz_pixels <= epsilon, dtype=tf.float32)\n",
        "  exponential_mask = tf.cast(fxfyfz_pixels > epsilon, dtype=tf.float32)\n",
        "  xyz_pixels = (3 * epsilon**2 * (fxfyfz_pixels - 4/29)) * linear_mask + (fxfyfz_pixels ** 3) * exponential_mask\n",
        "\n",
        "    # denormalize for D65 white point\n",
        "  xyz_pixels = tf.multiply(xyz_pixels, [0.950456, 1.0, 1.088754])\n",
        "\n",
        "      \n",
        "  xyz_to_rgb = tf.constant([\n",
        "      #     r           g          b\n",
        "      [ 3.2404542, -0.9692660,  0.0556434], # x\n",
        "      [-1.5371385,  1.8760108, -0.2040259], # y\n",
        "      [-0.4985314,  0.0415560,  1.0572252], # z\n",
        "  ])\n",
        "  rgb_pixels = tf.matmul(xyz_pixels, xyz_to_rgb)\n",
        "  # avoid a slightly negative number messing up the conversion\n",
        "  rgb_pixels = tf.clip_by_value(rgb_pixels, 0.0, 1.0)\n",
        "  linear_mask = tf.cast(rgb_pixels <= 0.0031308, dtype=tf.float32)\n",
        "  exponential_mask = tf.cast(rgb_pixels > 0.0031308, dtype=tf.float32)\n",
        "  srgb_pixels = (rgb_pixels * 12.92 * linear_mask) + ((rgb_pixels ** (1/2.4) * 1.055) - 0.055) * exponential_mask\n",
        "\n",
        "  return tf.reshape(srgb_pixels, tf.shape(lab))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Iy5SyOdXTx"
      },
      "source": [
        "def resize_with_ratio(img, size):\n",
        "  w, h = tf.shape(img)[0], tf.shape(img)[1]\n",
        " \n",
        "  new_height = h * size // tf.minimum(w, h)\n",
        "  new_width = w * size // tf.minimum(w, h)\n",
        "  \n",
        "  img = tf.image.resize(img, [new_width, new_height], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  return img\n",
        " \n",
        "def center_crop(img, size):\n",
        "  w, h = tf.shape(img)[0], tf.shape(img)[1]\n",
        "  x = (w - IMG_SIZE)//2\n",
        "  y = (h - IMG_SIZE)//2\n",
        "  img = tf.image.crop_to_bounding_box(img, x, y, IMG_SIZE, IMG_SIZE)\n",
        "  return img\n",
        " \n",
        "def normalise(lab_imgs):\n",
        "  imgs = tf.reshape(lab_imgs, [-1, 3])\n",
        "  normalised = (imgs / [50.0, 110.0, 110.0] ) - [1.0, 0.0, 0.0]\n",
        "  return tf.reshape(normalised, tf.shape(lab_imgs))\n",
        "\n",
        "def denormalise(lab_imgs):\n",
        "  imgs = tf.reshape(lab_imgs, [-1, 3])\n",
        "  unormalised = (imgs + [1.0, 0.0, 0.0]) * [50.0, 110.0, 110.0]\n",
        "  return tf.reshape(unormalised, tf.shape(lab_imgs))\n",
        "\n",
        " \n",
        "def data_augmentation(img):\n",
        "  img = tf.image.random_flip_left_right(img)\n",
        "  return img\n",
        "\n",
        "\n",
        "def post_process(L, AB, rescale=False):\n",
        "  image_lab = tf.concat([L, AB], axis=-1)\n",
        "  image_lab = denormalise(image_lab)\n",
        "  img_rgb = convert2rgb(image_lab)\n",
        "  if rescale:\n",
        "    img_rgb = img_rgb*255.0\n",
        "\n",
        "  return img_rgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqKJzF-5dp_S"
      },
      "source": [
        "def load(image_path):\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  img = tf.cast(img, dtype=tf.float32)/255.0\n",
        "  return img\n",
        " \n",
        "def load_image_train(file_path):\n",
        "  img = load(file_path)\n",
        "  img = tf.image.random_flip_left_right(img)\n",
        "  lab_img = convert2lab(img)\n",
        "  lab_img = normalise(lab_img)\n",
        "  L = lab_img[:,:,0][..., tf.newaxis]\n",
        "  AB = tf.stack([lab_img[:,:,1], lab_img[:,:,2]], axis=-1)\n",
        "  return L, AB, file_path\n",
        " \n",
        "#load an image and cretes the couple (L, RGB)\n",
        "def load_image_test(file_path):\n",
        "  #load image as RGB\n",
        "  img = load(file_path)\n",
        "  #Convert the image to LAB Space\n",
        "  lab_img = convert2lab(img)\n",
        "  #Nomalise the values so that they are in the range [-1, -1]\n",
        "  lab_img = normalise(lab_img)\n",
        "  #Extract the L channel\n",
        "  L = lab_img[:,:,0][..., tf.newaxis]\n",
        "  return L, img, file_path\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        " \n",
        "train_dataset = tf.data.Dataset.list_files('/content/Train/*.jpg')\n",
        "                                              \n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\\\n",
        "                            .map(load_image_train, num_parallel_calls=AUTOTUNE)\\\n",
        "                            .batch(BATCH_SIZE)\\\n",
        "                            .prefetch(AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.list_files('/content/Val/*.jpg', shuffle=False)\n",
        "                                              \n",
        "val_dataset = val_dataset.map(load_image_test)\\\n",
        "                          .batch(BATCH_SIZE)\\\n",
        "                          .prefetch(AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqjWABii_OSl"
      },
      "source": [
        "def down_sample(input, filters, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  x = Conv2D(filters, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False)(input)\n",
        "  if apply_batchnorm:\n",
        "    x = BatchNormalization()(x,training=True)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def up_sample(input, skip, filters, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  x = Conv2DTranspose(filters, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False)(input)\n",
        "  x = BatchNormalization()(x,training=True)\n",
        "  if apply_dropout:\n",
        "    x = Dropout(0.5)(x, training=True)\n",
        "  x = ReLU()(x)\n",
        "  x = Concatenate()([x, skip])\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjLXtuSafGPt"
      },
      "source": [
        "def define_generator(input_shape, output_channel, n_blocks):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  inputs = Input(input_shape)\n",
        "\n",
        "  min_filters = 64\n",
        "  \n",
        "  x = down_sample(inputs, 64, apply_batchnorm=False)\n",
        "  skips = [x]\n",
        "  nb_filters = [min_filters]\n",
        "  for i in range(1, n_blocks - 1):\n",
        "    filters = min_filters * min(8, (2 ** i))\n",
        "    x = down_sample(x, filters)\n",
        "    skips.append(x)\n",
        "    nb_filters.append(filters)\n",
        "  \n",
        "  x = down_sample(x, 512, apply_batchnorm=False)\n",
        "\n",
        "  nb_filters = reversed(nb_filters)\n",
        "  skips = reversed(skips)\n",
        "  for i, (filters, skip) in enumerate(zip(nb_filters, skips)):\n",
        "    apply_dp = i < 2\n",
        "    x = up_sample(x, skip, filters, apply_dropout=apply_dp)\n",
        "\n",
        "  output = Conv2DTranspose(output_channel, \n",
        "                          4, \n",
        "                          strides=2, \n",
        "                          padding='same', \n",
        "                          kernel_initializer=initializer,\n",
        "                          activation='tanh')(x) \n",
        "  \n",
        "  return Model(inputs=inputs, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwOKFUeRfWM5"
      },
      "source": [
        "def define_discriminator(input_shape, target_shape):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  inp = Input(shape=input_shape, name='input_image')\n",
        "  tar = Input(shape=target_shape, name='target_image')\n",
        "\n",
        "  x = Concatenate()([inp, tar])\n",
        "\n",
        "  x = down_sample(x, 64, apply_batchnorm=False)\n",
        "  x = down_sample(x, 128)\n",
        "  x = down_sample(x, 256) \n",
        "\n",
        "  x = ZeroPadding2D()(x)\n",
        "  x = Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(x)\n",
        "\n",
        "  x = BatchNormalization()(x, training=True)\n",
        "  x = LeakyReLU()(x)\n",
        "  x = ZeroPadding2D()(x)\n",
        "\n",
        "  last = Conv2D(1, 4, strides=1, kernel_initializer=initializer)(x)\n",
        "\n",
        "  return Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AICkxOnfmxo"
      },
      "source": [
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jrXLaeLj0d_"
      },
      "source": [
        "generator = define_generator([IMG_SIZE, IMG_SIZE, 1], OUTPUT_CHANNELS, int(log2(IMG_SIZE)))\n",
        "discriminator = define_discriminator([IMG_SIZE, IMG_SIZE, 1], [IMG_SIZE, IMG_SIZE, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ1TJGZKfMpg"
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        " \n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        " \n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        " \n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TvqEEmXf1tv"
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        " \n",
        "  generated_loss = loss_object(tf.ones_like(disc_generated_output)*-1.0, disc_generated_output)\n",
        " \n",
        "  total_disc_loss = real_loss + generated_loss\n",
        " \n",
        "  return total_disc_loss * 0.5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0eMDXlWf2OL"
      },
      "source": [
        "import datetime\n",
        "log_dir=\"logs/\"\n",
        " \n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tXgNh9RP7md"
      },
      "source": [
        "def generate_images(dataset, generator, save_path):\n",
        "  \n",
        "  def generate(dataset, generator):\n",
        "    batch_shape = tf.data.experimental.get_structure(dataset)[1].shapREAL_PATHe\n",
        "    fake_images = np.empty(shape=(0, IMG_SIZE, IMG_SIZE, 3))\n",
        "    inputs = np.empty(shape=(0, IMG_SIZE, IMG_SIZE, 1))\n",
        "    real_images = np.empty_like(fake_images)\n",
        "\n",
        "    for Ls, real, _ in dataset:\n",
        "      fake_ABs = generator.predict(Ls)\n",
        "\n",
        "      fake_RGBs = post_process(Ls, fake_ABs)\n",
        "\n",
        "      fake_images = np.append(fake_images, fake_RGBs, axis=0)\n",
        "      inputs = np.append(inputs, Ls, axis=0)\n",
        "      real_images = np.append(real_images, real, axis=0)\n",
        "\n",
        "    return inputs, real_images, fake_images \n",
        "\n",
        "  fig =  plt.figure(dpi=300, figsize=(30,30), constrained_layout=False)\n",
        "\n",
        "  inputs, real_images, fake_images = generate(dataset, generator)\n",
        "  num_images = fake_images.shape[0]\n",
        "\n",
        "  grid = ImageGrid(fig, 141,  #\n",
        "                     nrows_ncols=(num_images, 3),\n",
        "                     axes_pad=0.01,\n",
        "                     label_mode=\"1\",\n",
        "                     )\n",
        "\n",
        "  title = ['Input','Real', 'Fake']\n",
        "\n",
        "  for n, (L, real, fake) in enumerate(zip(inputs, real_images, fake_images)):\n",
        "    input = (L[..., 0] + 1.0)/2.0\n",
        "    display_list = [input , real, fake]\n",
        "\n",
        "    for i in range(3):\n",
        "      ax = grid[n*3 + i]\n",
        "      ax.axis('off')\n",
        "      if n == 0:\n",
        "        ax.set_title(title[i])\n",
        "      if i == 0:\n",
        "        ax.imshow(display_list[i], cmap='gray', aspect='auto')\n",
        "      else:\n",
        "        ax.imshow(display_list[i], aspect='auto')\n",
        "      \n",
        "  fig.savefig(save_path, bbox_inches='tight', aspect='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo5mQ3W8jtId"
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, epoch):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image)\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target])\n",
        "    disc_generated_output = discriminator([input_image, gen_output])\n",
        " \n",
        "    gen_total_loss, gen_gan_loss, l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        " \n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        " \n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        " \n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_l1_loss', l1_loss, step=epoch)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a3FdfZTk7J6"
      },
      "source": [
        "checkpoint_dir = '/content/drive/My Drive/VCS Project/checkpoints'\n",
        "checkpoints = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator,\n",
        "                                 step=tf.Variable(0))\n",
        "manager = tf.train.CheckpointManager(checkpoints, checkpoint_dir, max_to_keep=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0FbjAJoj-99"
      },
      "source": [
        "import datetime\n",
        "from datetime import timedelta\n",
        "import progressbar\n",
        "\n",
        "widgets=[\n",
        "    progressbar.Bar(marker='=', left='[', right=']'),\n",
        "    progressbar.widgets.SimpleProgress(),\n",
        "    ' - ', progressbar.Timer(), '',\n",
        "    '-', progressbar.AdaptiveETA(), ''\n",
        "]\n",
        "def fit(train_ds, epochs):\n",
        "  checkpoints.restore(manager.latest_checkpoint)\n",
        "\n",
        "  if manager.latest_checkpoint:\n",
        "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "  else:\n",
        "    print(\"Initializing from scratch.\")\n",
        "\n",
        "  step = int(checkpoints.step)\n",
        "  \n",
        "  for epoch in range(step, epochs):\n",
        "    start = time.time()\n",
        "    iters = train_ds.cardinality().numpy()\n",
        " \n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    # Train\n",
        "    with progressbar.ProgressBar(max_value=iters, widgets=widgets) as pbar:\n",
        "      for n, (input_image, target, _) in train_ds.enumerate():\n",
        "    \n",
        "        train_step(input_image, target, epoch)\n",
        "        pbar.update(n.numpy())\n",
        "\n",
        "    path = f'/content/drive/My Drive/VCS Project/samples/sample_{epoch + 1}.jpg'\n",
        "    generate_images(val_dataset, generator , path);\n",
        "\n",
        "    checkpoints.step.assign_add(1)\n",
        "    #saving (checkpoint) the model after 10 epochs\n",
        "    if epoch >= 10:\n",
        "      manager.save()\n",
        "\n",
        "    clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayc7S9hWIIOl"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ainz9burlNU9"
      },
      "source": [
        "fit(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVMEkgZZhaaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b767d5-502d-4e29-b987-f12d7e508005"
      },
      "source": [
        "checkpoints.restore('/content/drive/My Drive/VCS Project/Best Model/ckpt-97')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbf3ff597f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qM7pHucDUXI"
      },
      "source": [
        "path = f'/content/drive/My Drive/VCS Project/sample.jpg'\n",
        "generate_images(val_dataset, generator , path);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTu3c7ewBDz-",
        "outputId": "f00b6599-3803-4fc0-e2ca-08c825e50139"
      },
      "source": [
        "generator.save('/content/drive/My Drive/VCS Project/Best model/', include_optimizer=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/VCS Project/Best model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJnI8dtVTYWg"
      },
      "source": [
        "shutil.copy(\"/content/drive/My Drive/VCS Project/Test.zip\", \".\")\n",
        "!unzip -q Test.zip\n",
        "!rm Test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk7SjqZxlbG5"
      },
      "source": [
        "test_paths = tf.data.Dataset.list_files('/content/Test/*.jpg', shuffle=False)\n",
        "test_dataset = test_paths.map(load_image_test, deterministic=True)\\\n",
        "                          .batch(BATCH_SIZE)\\\n",
        "                          .prefetch(AUTOTUNE)\n",
        "\n",
        "for Ls, _, file_paths in test_dataset:\n",
        "\n",
        "  fake_ABs = generator.predict(Ls)\n",
        "\n",
        "  fake_LABs = tf.concat([Ls, fake_ABs], axis=-1)\n",
        "  fake_LABs = denormalise(fake_LABs)\n",
        "  fake_RGBs = convert2rgb(fake_LABs)\n",
        "\n",
        "  for (fake, file_path) in zip(fake_RGBs, file_paths):\n",
        "    fake_img = tf.image.convert_image_dtype(fake, dtype=tf.uint8)\n",
        "    img = tf.image.encode_jpeg(fake_img)\n",
        "\n",
        "    filename = os.path.basename(file_path.numpy())\n",
        "    save_path= f'/content/drive/My Drive/VCS Project/examples/{filename.decode(\"utf-8\")}'\n",
        "    tf.io.write_file(save_path, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgzjIkPVj-E-"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZMw1fUMyssI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}